{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKWX90pG2XPo"
      },
      "source": [
        "## Creating The Model\n",
        "\n",
        "Write One Soft Tree From Scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YWoceKCP2g90"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class SoftTreeEnsemble(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_trees, max_depth, leaf_dims, input_dim, activation='sigmoid', node_index=0,\n",
        "                 internal_eps = 0, combine_output = True, subset_selection = False, device='cpu'):\n",
        "        \"\"\"\n",
        "          # The max depth will determine the number of nodes that we have\n",
        "          # This will be 2^{d+1} - 1 where d is the edges from root to leaf\n",
        "          # This will be the dimensionality of the vector returned by the tree\n",
        "          # should match the number of classes in the problem but can be more\n",
        "          # if more then pass through final activation to resize\n",
        "          # can only use this if we output raw score\n",
        "          # s1.right_child.right_child.node_index\n",
        "              This will grab the node from root, to the right_child, and to that right child\n",
        "\n",
        "\n",
        "          # if combine_output is true we get (batch_size, leaf_dim)\n",
        "          # if false then we get (batch_size, num_trees, leaf_dim)\n",
        "\n",
        "          # subset selection:\n",
        "              This will be a boolean value that if true we perform the randomization of feature selection in a random forrest\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        super(SoftTreeEnsemble, self).__init__()\n",
        "        self.num_trees = num_trees\n",
        "        self.combine_output = combine_output\n",
        "        self.max_depth = max_depth\n",
        "        self.leaf_dims = leaf_dims\n",
        "        self.activation = activation\n",
        "        self.node_index = node_index\n",
        "        self.internal_eps = internal_eps\n",
        "        self.leaf = node_index >= 2**max_depth-1\n",
        "        self.subset_selection = subset_selection\n",
        "        self.device = device\n",
        "        self.batch_norm = nn.BatchNorm1d(self.leaf_dims)\n",
        "\n",
        "\n",
        "        # instatiate through recursion\n",
        "        # we will build the tree structure first\n",
        "        if not self.leaf:\n",
        "\n",
        "          # for each node we need to make a FC layer w'x\n",
        "          # takes as input the input dimension\n",
        "          # and outputs the probabilities for each class\n",
        "          # output dim is one to resemble a tabular data structure\n",
        "\n",
        "          # to extend this to multiple trees we can do the following\n",
        "          # we have each level of the nodes but now extended to the number of\n",
        "          # trees\n",
        "\n",
        "\n",
        "          self.fc = nn.Linear(input_dim, num_trees)\n",
        "\n",
        "          # we need to choose a feature of values\n",
        "          if self.subset_selection:\n",
        "            # create a matrix of the same row as the rows in W\n",
        "            # for now will not be part of the gradient computation\n",
        "            # we will keep sqrt(p) for now\n",
        "            num_features = math.floor(math.sqrt(input_dim))\n",
        "\n",
        "\n",
        "            temp = torch.ones(input_dim, requires_grad=False).to(self.device)\n",
        "\n",
        "            zero_indices = torch.randperm(temp.shape[0])[:num_features]\n",
        "            temp[zero_indices] = 0\n",
        "\n",
        "            self.mask = nn.Parameter(temp, requires_grad=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # builds out the left and the right child for a balanced tree\n",
        "          # also gives is a node index\n",
        "          self.left_child = SoftTreeEnsemble(\n",
        "              num_trees, max_depth, leaf_dims,\n",
        "              input_dim, activation, 2*node_index+1 ,\n",
        "              combine_output = self.combine_output, subset_selection = self.subset_selection,\n",
        "              device=self.device\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "          self.right_child = SoftTreeEnsemble(\n",
        "              num_trees, max_depth, leaf_dims, input_dim ,activation,\n",
        "              2*node_index+2,combine_output = self.combine_output,\n",
        "              subset_selection = self.subset_selection, device= self.device\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "        else:\n",
        "          # creates weights for the leaf nodes for voting\n",
        "          # we also need to add this for the multiple trees\n",
        "          self.leaf_weights = nn.Parameter(\n",
        "              torch.randn(1, self.leaf_dims, self.num_trees, requires_grad=True).to(self.device) * 0.1\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # then we call the actual forward pass of the tree\n",
        "    def forward(self, x, prob=1.0):\n",
        "        \"\"\"\n",
        "            This runs the forward class of the model\n",
        "        \"\"\"\n",
        "\n",
        "        # we first check if it is a leaf or not\n",
        "        if not self.leaf:\n",
        "          # apply the hadamard\n",
        "          if self.subset_selection:\n",
        "            masked_weights = self.fc.weight * self.mask\n",
        "\n",
        "            current_prob = torch.clamp(torch.sigmoid(F.linear(x, masked_weights, self.fc.bias)), min=self.internal_eps, max=1-self.internal_eps)\n",
        "\n",
        "          else:\n",
        "            # we make sure that the decision is not hard 1 or 0\n",
        "            current_prob = torch.clamp(torch.sigmoid(self.fc(x)), min=self.internal_eps, max=1-self.internal_eps)\n",
        "            # return the probability for going to the left or the right\n",
        "          return self.left_child(x, prob*current_prob) + self.right_child(x, prob*(1-current_prob))\n",
        "\n",
        "          # what do we do when we get to a leaf\n",
        "        else:\n",
        "\n",
        "\n",
        "          # element wise product between the probability that the data point gets to the leaf\n",
        "          # to the weight that the leaf has\n",
        "          # enable prob to broad cast\n",
        "          output = prob.unsqueeze(1) * self.leaf_weights\n",
        "\n",
        "\n",
        "          if self.combine_output:\n",
        "            output = torch.sum(output, dim=2)\n",
        "            # rint(output)\n",
        "\n",
        "          output = self.batch_norm(output)\n",
        "\n",
        "          # assert len(output.shape) == 3\n",
        "          return F.softmax(output, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InI_rBqdtYBl"
      },
      "source": [
        "## Grabbing The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pmlb import dataset_names,fetch_data\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import xgboost as xgb\n",
        "# import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7gGqtmxtbz5",
        "outputId": "40432688-678b-4fd3-d725-1a148816ad02"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:921\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:813\u001b[0m, in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:1288\u001b[0m, in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# The 23 datasets used in TEL\u001b[39;00m\n\u001b[0;32m     16\u001b[0m pmlb_datasets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mann_thyroid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreast_cancer\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcar_evaluation\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchurn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdermatology\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiabetes\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdna\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecoli\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflare\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheart_c\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhypothyroid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnursery\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptdigits\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpima\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msatimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msleep\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolar_flare_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspambase\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexture\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwonorm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvehicle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myeast\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\azpee\\anaconda3\\envs\\Torch\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
            "File \u001b[1;32mc:\\Users\\azpee\\anaconda3\\envs\\Torch\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1334\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "######################################### GOAL #################################\n",
        "# Put all of the datasets in a hdf5 file.\n",
        "# Makes it super easy to grab whatever we want\n",
        "################################################################################\n",
        "\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pmlb import dataset_names,fetch_data\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# The 23 datasets used in TEL\n",
        "pmlb_datasets = ['ann_thyroid', 'breast_cancer','car_evaluation','churn', 'crx', 'dermatology', 'diabetes',\n",
        "                 'dna', 'ecoli', 'flare', 'heart_c', 'hypothyroid', 'nursery', 'optdigits', 'pima', 'satimage', 'sleep',\n",
        "                 'solar_flare_2', 'spambase', 'texture', 'twonorm', 'vehicle', 'yeast']\n",
        "\n",
        "# ones that need to be checked\n",
        "failed_datasets = []\n",
        "\n",
        "# Create an HDF5 file\n",
        "with h5py.File('datasets.h5', 'w') as h5file:\n",
        "    for classification_data_set in pmlb_datasets:\n",
        "\n",
        "        print(f\"Processing dataset: {classification_data_set}\")\n",
        "\n",
        "        try:\n",
        "          # fetch the data\n",
        "          X, y = fetch_data(classification_data_set, return_X_y=True)\n",
        "        except:\n",
        "          print(f\"Dataset {classification_data_set} failed to load\")\n",
        "          failed_datasets.append(classification_data_set)\n",
        "          continue\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "\n",
        "        train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.7)\n",
        "\n",
        "        # Create a group for each dataset\n",
        "        group = h5file.create_group(classification_data_set)\n",
        "\n",
        "        # Save train and test data\n",
        "        group.create_dataset('train_X', data=train_X)\n",
        "        group.create_dataset('test_X', data=test_X)\n",
        "        group.create_dataset('train_y', data=train_y)\n",
        "        group.create_dataset('test_y', data=test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vhd3dNnPuB44"
      },
      "outputs": [],
      "source": [
        "# Write a function to retrieve the data set\n",
        "\n",
        "def load_data(dataset_name, framework='sklearn', file_path='datasets.h5', batch_size=32):\n",
        "  '''\n",
        "    Function Goal:\n",
        "      Loads in data from the hdf5 file containing all datasets\n",
        "    Parameters:\n",
        "      dataset_name: Name of the data set\n",
        "      framework:\n",
        "        sklearn --> numpy arrays {train_X, test_X, train_y, test_y}\n",
        "        torch --> (train_loader, test_loader)\n",
        "        boost --> (train, test)\n",
        "        tf --> (train, test)\n",
        "  '''\n",
        "\n",
        "    # Open the HDF5 file\n",
        "  with h5py.File(file_path, 'r') as h5file:\n",
        "        # Navigate to the group (folder) for the specified dataset\n",
        "      try:\n",
        "        group = h5file[dataset_name]\n",
        "      except KeyError:\n",
        "        raise ValueError(f\"Dataset '{dataset_name}' not found in the HDF5 file.\")\n",
        "\n",
        "        # Load train and test data as NumPy arrays\n",
        "      train_X = group['train_X'][:]\n",
        "      test_X = group['test_X'][:]\n",
        "      train_y = group['train_y'][:]\n",
        "      test_y = group['test_y'][:]\n",
        "\n",
        "    # Format data based on the specified framework\n",
        "  if framework == 'sklearn':\n",
        "        # Return NumPy arrays (compatible with scikit-learn)\n",
        "      return {'train_X':train_X, 'test_X':test_X, 'train_y':train_y, 'test_y':test_y}\n",
        "\n",
        "  elif framework == 'torch':\n",
        "      # Convert to PyTorch tensors\n",
        "      train_dataset = TensorDataset(torch.tensor(train_X, dtype=torch.float32),\n",
        "                                      torch.tensor(train_y, dtype=torch.long))\n",
        "      test_dataset = TensorDataset(torch.tensor(test_X, dtype=torch.float32),\n",
        "                                     torch.tensor(test_y, dtype=torch.long))\n",
        "\n",
        "        # Create DataLoaders for batching\n",
        "      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "      test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "      return train_loader, test_loader\n",
        "\n",
        "  elif framework == 'boost':\n",
        "        # Convert to DMatrix format (XGBoost's preferred format)\n",
        "      train_dmatrix = xgb.DMatrix(train_X, label=train_y)\n",
        "      test_dmatrix = xgb.DMatrix(test_X, label=test_y)\n",
        "      return train_dmatrix, test_dmatrix\n",
        "\n",
        "  # elif framework == 'tf':\n",
        "        # Create TensorFlow datasets\n",
        "      train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
        "      test_dataset = tf.data.Dataset.from_tensor_slices((test_X, test_y))\n",
        "\n",
        "        # Batch and shuffle data\n",
        "      train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "      test_dataset = test_dataset.batch(batch_size)\n",
        "      return train_dataset, test_dataset\n",
        "\n",
        "  else:\n",
        "      raise ValueError(\"Unsupported framework. Choose from 'sklearn', 'pytorch', 'xgboost', or 'tensorflow'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TVjxFhNwvMI6"
      },
      "outputs": [],
      "source": [
        "# grab the breast data set\n",
        "data = load_data('breast_cancer', framework='torch', batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av2OHTZByZtO",
        "outputId": "0ad9994b-a1f4-47c9-9cc9-9625fbc4ee66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7a89fd065f50>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7a89fd6e1b50>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txJHO6r8EM_r"
      },
      "source": [
        "### Understanding the Weight Matricies\n",
        "\n",
        "The weights are goverened as the following. For every node level like the root, we have that there will be (num_trees, num_features) for that weight matrix. For the leaf weights they will be shaped as the following. (leaf_dims, num_trees).\n",
        "\n",
        "### Training\n",
        "* Must always follow batch normalizaition.\n",
        "* must use cross entropy loss\n",
        "* Looking at the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJmQW3-i6NjC"
      },
      "outputs": [],
      "source": [
        "# first make the model\n",
        "model = SoftTreeEnsemble(num_trees=3,max_depth=2, leaf_dims=5, input_dim=9, combine_output=True, subset_selection=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2kP6xrbv8ej"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "################################################################################\n",
        "# GOAL: Train the model\n",
        "################################################################################\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=10, learning_rate=0.001, device='cpu'):\n",
        "    # Define the optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()  # Use appropriate loss function for your task\n",
        "\n",
        "    # Move model to device (GPU or CPU)\n",
        "    model.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            # Move data to device (GPU or CPU)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Track accuracy or other metrics if needed\n",
        "            # For classification, you can calculate accuracy\n",
        "            predicted = outputs.argmax(dim=1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        # accuracy = correct_preds / total_preds (if classification)\n",
        "\n",
        "        test_loss = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_loss:.4f} | Test Lost: {test_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(test_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCtAhKYb9QzH",
        "outputId": "e5072a23-ea1d-4cb1-805e-439a652ed67f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Training Loss: 1.0626 | Test Lost: 1.1231\n",
            "Epoch [2/5], Training Loss: 1.0222 | Test Lost: 1.0996\n",
            "Epoch [3/5], Training Loss: 0.9952 | Test Lost: 1.0878\n",
            "Epoch [4/5], Training Loss: 1.0068 | Test Lost: 1.0847\n",
            "Epoch [5/5], Training Loss: 1.0093 | Test Lost: 1.0612\n"
          ]
        }
      ],
      "source": [
        "train_model(model, data[0], data[1], epochs=5, learning_rate=0.001, device='cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLA7p2KN5fcd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "hyperparameters = {\n",
        "            \"learning_rate\": np.logspace(-1, -5, 5),\n",
        "            \"batch_size\": [16, 32],\n",
        "            \"num_epochs\": range(5,20),\n",
        "            \"tree_depth\": range(2,15)\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ud0ZjmaENHl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Random grid search function\n",
        "def random_grid_search(model, hyperparameters, train_loader, test_loader, epochs=10, device='cpu', n_iter=10):\n",
        "    \"\"\"\n",
        "    Perform random grid search on hyperparameters and train the model.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model.\n",
        "        hyperparameters: Dictionary of hyperparameters to search over.\n",
        "        train_loader: DataLoader for training.\n",
        "        test_loader: DataLoader for testing.\n",
        "        epochs: Number of epochs to train the model.\n",
        "        device: Device to use ('cpu' or 'cuda').\n",
        "        n_iter: Number of random hyperparameter combinations to try.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate all possible hyperparameter combinations (for reference)\n",
        "    hyperparameter_keys = list(hyperparameters.keys())\n",
        "\n",
        "    # Randomly sample n_iter combinations\n",
        "    for i in range(n_iter):\n",
        "        # Randomly select hyperparameters\n",
        "\n",
        "        sample = {key: random.choice(hyperparameters[key]) for key in hyperparameter_keys}\n",
        "\n",
        "        # Print the combination being used\n",
        "        print(f\"Trial {i+1}/{n_iter}: Hyperparameters - {sample}\")\n",
        "\n",
        "        # Extract the sampled values\n",
        "\n",
        "        learning_rate = sample[\"learning_rate\"]\n",
        "        batch_size = sample[\"batch_size\"]\n",
        "        num_epochs = sample[\"num_epochs\"]\n",
        "        tree_depth = sample[\"tree_depth\"]\n",
        "        # num_trees = sample[\"num_trees\"]\n",
        "\n",
        "        # first thing is load the data\n",
        "        data = load_data('breast_cancer', framework='torch', batch_size=batch_size)\n",
        "\n",
        "        # create the instance of the model\n",
        "        model = SoftTreeEnsemble(num_trees=1,max_depth=tree_depth,\n",
        "                                 leaf_dims=2, input_dim=9, combine_output=True, subset_selection=True)\n",
        "\n",
        "        # Train the model with the current hyperparameters\n",
        "        print(f\"Training model with learning rate: {learning_rate}, batch size: {batch_size}, \"\n",
        "              f\"epochs: {num_epochs}, tree depth: {tree_depth}\")\n",
        "\n",
        "        # Call the training function (your existing train_model function)\n",
        "        train_model(model, train_loader, test_loader, epochs=num_epochs, learning_rate=learning_rate, device=device)\n",
        "\n",
        "        # Add any additional evaluation or result saving here if needed\n",
        "        print(f\"Completed Trial {i+1}/{n_iter}.\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlBodueQFR-_",
        "outputId": "b37d345b-3cc5-4599-c7b3-483e8ab23dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1/10: Hyperparameters - {'learning_rate': 9.999999999999999e-05, 'batch_size': 32, 'num_epochs': 10, 'tree_depth': 8}\n",
            "Training model with learning rate: 9.999999999999999e-05, batch size: 32, epochs: 10, tree depth: 8\n",
            "Epoch [1/10], Training Loss: 0.7311 | Test Lost: 0.6572\n",
            "Epoch [2/10], Training Loss: 0.6478 | Test Lost: 0.6452\n",
            "Epoch [3/10], Training Loss: 0.6284 | Test Lost: 0.6452\n",
            "Epoch [4/10], Training Loss: 0.5940 | Test Lost: 0.6502\n",
            "Epoch [5/10], Training Loss: 0.5816 | Test Lost: 0.6527\n",
            "Epoch [6/10], Training Loss: 0.5853 | Test Lost: 0.6546\n",
            "Epoch [7/10], Training Loss: 0.5861 | Test Lost: 0.6508\n",
            "Epoch [8/10], Training Loss: 0.5740 | Test Lost: 0.6442\n",
            "Epoch [9/10], Training Loss: 0.5720 | Test Lost: 0.6339\n",
            "Epoch [10/10], Training Loss: 0.5507 | Test Lost: 0.6324\n",
            "Completed Trial 1/10.\n",
            "\n",
            "Trial 2/10: Hyperparameters - {'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 12, 'tree_depth': 5}\n",
            "Training model with learning rate: 0.001, batch size: 32, epochs: 12, tree depth: 5\n",
            "Epoch [1/12], Training Loss: 0.8100 | Test Lost: 0.6513\n",
            "Epoch [2/12], Training Loss: 0.6430 | Test Lost: 0.6395\n",
            "Epoch [3/12], Training Loss: 0.5986 | Test Lost: 0.6391\n",
            "Epoch [4/12], Training Loss: 0.5707 | Test Lost: 0.6313\n",
            "Epoch [5/12], Training Loss: 0.5349 | Test Lost: 0.6132\n",
            "Epoch [6/12], Training Loss: 0.5505 | Test Lost: 0.6017\n",
            "Epoch [7/12], Training Loss: 0.5166 | Test Lost: 0.5775\n",
            "Epoch [8/12], Training Loss: 0.5285 | Test Lost: 0.5688\n",
            "Epoch [9/12], Training Loss: 0.5310 | Test Lost: 0.5593\n",
            "Epoch [10/12], Training Loss: 0.5272 | Test Lost: 0.5781\n",
            "Epoch [11/12], Training Loss: 0.5024 | Test Lost: 0.5877\n",
            "Epoch [12/12], Training Loss: 0.5142 | Test Lost: 0.5874\n",
            "Completed Trial 2/10.\n",
            "\n",
            "Trial 3/10: Hyperparameters - {'learning_rate': 0.09999999999999999, 'batch_size': 16, 'num_epochs': 12, 'tree_depth': 6}\n",
            "Training model with learning rate: 0.09999999999999999, batch size: 16, epochs: 12, tree depth: 6\n",
            "Epoch [1/12], Training Loss: 0.8706 | Test Lost: 1.3778\n",
            "Epoch [2/12], Training Loss: 0.9769 | Test Lost: 1.2211\n",
            "Epoch [3/12], Training Loss: 0.6401 | Test Lost: 0.6424\n",
            "Epoch [4/12], Training Loss: 0.6070 | Test Lost: 0.9150\n",
            "Epoch [5/12], Training Loss: 0.6360 | Test Lost: 0.7407\n",
            "Epoch [6/12], Training Loss: 0.6146 | Test Lost: 0.5819\n",
            "Epoch [7/12], Training Loss: 0.6251 | Test Lost: 0.6459\n",
            "Epoch [8/12], Training Loss: 0.5965 | Test Lost: 0.8226\n",
            "Epoch [9/12], Training Loss: 0.5985 | Test Lost: 0.7121\n",
            "Epoch [10/12], Training Loss: 0.7615 | Test Lost: 1.0465\n",
            "Epoch [11/12], Training Loss: 0.8524 | Test Lost: 0.6590\n",
            "Epoch [12/12], Training Loss: 0.6698 | Test Lost: 0.8044\n",
            "Completed Trial 3/10.\n",
            "\n",
            "Trial 4/10: Hyperparameters - {'learning_rate': 0.01, 'batch_size': 16, 'num_epochs': 19, 'tree_depth': 8}\n",
            "Training model with learning rate: 0.01, batch size: 16, epochs: 19, tree depth: 8\n",
            "Epoch [1/19], Training Loss: 0.6772 | Test Lost: 0.7748\n",
            "Epoch [2/19], Training Loss: 0.5505 | Test Lost: 1.2184\n",
            "Epoch [3/19], Training Loss: 0.6538 | Test Lost: 0.8475\n",
            "Epoch [4/19], Training Loss: 0.5944 | Test Lost: 0.6240\n",
            "Epoch [5/19], Training Loss: 0.6907 | Test Lost: 0.6418\n",
            "Epoch [6/19], Training Loss: 0.6620 | Test Lost: 0.8835\n",
            "Epoch [7/19], Training Loss: 0.5489 | Test Lost: 0.8789\n",
            "Epoch [8/19], Training Loss: 0.6898 | Test Lost: 0.6386\n",
            "Epoch [9/19], Training Loss: 0.7041 | Test Lost: 0.5651\n",
            "Epoch [10/19], Training Loss: 0.5541 | Test Lost: 0.7011\n",
            "Epoch [11/19], Training Loss: 0.5571 | Test Lost: 0.7250\n",
            "Epoch [12/19], Training Loss: 0.5451 | Test Lost: 0.6550\n",
            "Epoch [13/19], Training Loss: 0.4187 | Test Lost: 0.6996\n",
            "Epoch [14/19], Training Loss: 0.4246 | Test Lost: 0.6362\n",
            "Epoch [15/19], Training Loss: 0.4240 | Test Lost: 0.7366\n",
            "Epoch [16/19], Training Loss: 0.4580 | Test Lost: 0.6583\n",
            "Epoch [17/19], Training Loss: 0.6103 | Test Lost: 0.6578\n",
            "Epoch [18/19], Training Loss: 0.5343 | Test Lost: 0.6319\n",
            "Epoch [19/19], Training Loss: 0.4041 | Test Lost: 0.6873\n",
            "Completed Trial 4/10.\n",
            "\n",
            "Trial 5/10: Hyperparameters - {'learning_rate': 9.999999999999999e-05, 'batch_size': 16, 'num_epochs': 8, 'tree_depth': 2}\n",
            "Training model with learning rate: 9.999999999999999e-05, batch size: 16, epochs: 8, tree depth: 2\n",
            "Epoch [1/8], Training Loss: 0.7528 | Test Lost: 0.6991\n",
            "Epoch [2/8], Training Loss: 0.7546 | Test Lost: 0.7049\n",
            "Epoch [3/8], Training Loss: 0.7620 | Test Lost: 0.7168\n",
            "Epoch [4/8], Training Loss: 0.7679 | Test Lost: 0.7381\n",
            "Epoch [5/8], Training Loss: 0.7520 | Test Lost: 0.7800\n",
            "Epoch [6/8], Training Loss: 0.7508 | Test Lost: 0.8167\n",
            "Epoch [7/8], Training Loss: 0.7529 | Test Lost: 0.8325\n",
            "Epoch [8/8], Training Loss: 0.7621 | Test Lost: 0.8348\n",
            "Completed Trial 5/10.\n",
            "\n",
            "Trial 6/10: Hyperparameters - {'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 9, 'tree_depth': 14}\n",
            "Training model with learning rate: 0.01, batch size: 32, epochs: 9, tree depth: 14\n",
            "Epoch [1/9], Training Loss: 11.7878 | Test Lost: 2.6367\n",
            "Epoch [2/9], Training Loss: 3.8767 | Test Lost: 15.2722\n",
            "Epoch [3/9], Training Loss: 9.8231 | Test Lost: 14.1809\n",
            "Epoch [4/9], Training Loss: 6.6689 | Test Lost: 8.8020\n",
            "Epoch [5/9], Training Loss: 3.0130 | Test Lost: 2.4518\n",
            "Epoch [6/9], Training Loss: 2.0654 | Test Lost: 15.8444\n",
            "Epoch [7/9], Training Loss: 12.3814 | Test Lost: 13.7426\n",
            "Epoch [8/9], Training Loss: 8.6848 | Test Lost: 7.1202\n",
            "Epoch [9/9], Training Loss: 12.6846 | Test Lost: 11.4128\n",
            "Completed Trial 6/10.\n",
            "\n",
            "Trial 7/10: Hyperparameters - {'learning_rate': 9.999999999999999e-05, 'batch_size': 16, 'num_epochs': 15, 'tree_depth': 7}\n",
            "Training model with learning rate: 9.999999999999999e-05, batch size: 16, epochs: 15, tree depth: 7\n",
            "Epoch [1/15], Training Loss: 0.6968 | Test Lost: 0.6716\n",
            "Epoch [2/15], Training Loss: 0.6649 | Test Lost: 0.6563\n",
            "Epoch [3/15], Training Loss: 0.6336 | Test Lost: 0.6444\n",
            "Epoch [4/15], Training Loss: 0.6290 | Test Lost: 0.6337\n",
            "Epoch [5/15], Training Loss: 0.6179 | Test Lost: 0.6203\n",
            "Epoch [6/15], Training Loss: 0.6033 | Test Lost: 0.5995\n",
            "Epoch [7/15], Training Loss: 0.5897 | Test Lost: 0.5668\n",
            "Epoch [8/15], Training Loss: 0.5764 | Test Lost: 0.5355\n",
            "Epoch [9/15], Training Loss: 0.5722 | Test Lost: 0.5165\n",
            "Epoch [10/15], Training Loss: 0.5680 | Test Lost: 0.5101\n",
            "Epoch [11/15], Training Loss: 0.5564 | Test Lost: 0.5120\n",
            "Epoch [12/15], Training Loss: 0.5514 | Test Lost: 0.5106\n",
            "Epoch [13/15], Training Loss: 0.5573 | Test Lost: 0.5207\n",
            "Epoch [14/15], Training Loss: 0.5674 | Test Lost: 0.5197\n",
            "Epoch [15/15], Training Loss: 0.5362 | Test Lost: 0.5150\n",
            "Completed Trial 7/10.\n",
            "\n",
            "Trial 8/10: Hyperparameters - {'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 8, 'tree_depth': 8}\n",
            "Training model with learning rate: 0.01, batch size: 32, epochs: 8, tree depth: 8\n",
            "Epoch [1/8], Training Loss: 1.0640 | Test Lost: 0.8671\n",
            "Epoch [2/8], Training Loss: 0.6273 | Test Lost: 0.6610\n",
            "Epoch [3/8], Training Loss: 0.6364 | Test Lost: 0.8688\n",
            "Epoch [4/8], Training Loss: 0.6289 | Test Lost: 0.7474\n",
            "Epoch [5/8], Training Loss: 0.5258 | Test Lost: 0.6887\n",
            "Epoch [6/8], Training Loss: 0.6511 | Test Lost: 0.8214\n",
            "Epoch [7/8], Training Loss: 0.7319 | Test Lost: 1.4164\n",
            "Epoch [8/8], Training Loss: 0.7135 | Test Lost: 0.9593\n",
            "Completed Trial 8/10.\n",
            "\n",
            "Trial 9/10: Hyperparameters - {'learning_rate': 9.999999999999999e-05, 'batch_size': 16, 'num_epochs': 15, 'tree_depth': 5}\n",
            "Training model with learning rate: 9.999999999999999e-05, batch size: 16, epochs: 15, tree depth: 5\n",
            "Epoch [1/15], Training Loss: 0.7817 | Test Lost: 0.6897\n",
            "Epoch [2/15], Training Loss: 0.7372 | Test Lost: 0.6851\n",
            "Epoch [3/15], Training Loss: 0.7309 | Test Lost: 0.6813\n",
            "Epoch [4/15], Training Loss: 0.7123 | Test Lost: 0.6784\n",
            "Epoch [5/15], Training Loss: 0.6893 | Test Lost: 0.6774\n",
            "Epoch [6/15], Training Loss: 0.6768 | Test Lost: 0.6790\n",
            "Epoch [7/15], Training Loss: 0.6595 | Test Lost: 0.6927\n",
            "Epoch [8/15], Training Loss: 0.6475 | Test Lost: 0.7027\n",
            "Epoch [9/15], Training Loss: 0.6339 | Test Lost: 0.6946\n",
            "Epoch [10/15], Training Loss: 0.6258 | Test Lost: 0.6772\n",
            "Epoch [11/15], Training Loss: 0.6243 | Test Lost: 0.6665\n",
            "Epoch [12/15], Training Loss: 0.6113 | Test Lost: 0.6579\n",
            "Epoch [13/15], Training Loss: 0.6253 | Test Lost: 0.6469\n",
            "Epoch [14/15], Training Loss: 0.6059 | Test Lost: 0.6419\n",
            "Epoch [15/15], Training Loss: 0.6030 | Test Lost: 0.6352\n",
            "Completed Trial 9/10.\n",
            "\n",
            "Trial 10/10: Hyperparameters - {'learning_rate': 0.09999999999999999, 'batch_size': 32, 'num_epochs': 17, 'tree_depth': 5}\n",
            "Training model with learning rate: 0.09999999999999999, batch size: 32, epochs: 17, tree depth: 5\n",
            "Epoch [1/17], Training Loss: 0.8356 | Test Lost: 0.7884\n",
            "Epoch [2/17], Training Loss: 0.5877 | Test Lost: 1.0033\n",
            "Epoch [3/17], Training Loss: 0.6468 | Test Lost: 0.8857\n",
            "Epoch [4/17], Training Loss: 0.6795 | Test Lost: 0.6068\n",
            "Epoch [5/17], Training Loss: 0.5546 | Test Lost: 0.5676\n",
            "Epoch [6/17], Training Loss: 0.5967 | Test Lost: 0.5320\n",
            "Epoch [7/17], Training Loss: 0.5862 | Test Lost: 0.5233\n",
            "Epoch [8/17], Training Loss: 0.5866 | Test Lost: 0.5941\n",
            "Epoch [9/17], Training Loss: 0.5443 | Test Lost: 0.8838\n",
            "Epoch [10/17], Training Loss: 0.6645 | Test Lost: 1.3131\n",
            "Epoch [11/17], Training Loss: 0.6928 | Test Lost: 0.7385\n",
            "Epoch [12/17], Training Loss: 0.6634 | Test Lost: 0.6655\n",
            "Epoch [13/17], Training Loss: 0.6203 | Test Lost: 0.5737\n",
            "Epoch [14/17], Training Loss: 0.6824 | Test Lost: 0.9032\n",
            "Epoch [15/17], Training Loss: 0.8850 | Test Lost: 0.7479\n",
            "Epoch [16/17], Training Loss: 0.7669 | Test Lost: 1.0090\n",
            "Epoch [17/17], Training Loss: 0.7111 | Test Lost: 0.7186\n",
            "Completed Trial 10/10.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "random_grid_search(model, hyperparameters, data[0], data[1], epochs=10, device='cpu', n_iter=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YERT0EIjSOWc",
        "outputId": "c198890c-a436-4bb9-a08b-d55f62976b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1/10: Hyperparameters - {'learning_rate': 0.001, 'batch_size': 16, 'num_epochs': 6, 'tree_depth': 14}\n",
            "Training model with learning rate: 0.001, batch size: 16, epochs: 6, tree depth: 14\n",
            "Epoch [1/6], Training Loss: 2.7794 | Test Lost: 2.6957\n",
            "Epoch [2/6], Training Loss: 1.0208 | Test Lost: 0.7694\n",
            "Epoch [3/6], Training Loss: 0.5884 | Test Lost: 1.2189\n",
            "Epoch [4/6], Training Loss: 0.6640 | Test Lost: 1.0855\n",
            "Epoch [5/6], Training Loss: 0.6841 | Test Lost: 1.7482\n",
            "Epoch [6/6], Training Loss: 1.5776 | Test Lost: 2.2663\n",
            "Completed Trial 1/10.\n",
            "\n",
            "Trial 2/10: Hyperparameters - {'learning_rate': 9.999999999999999e-06, 'batch_size': 16, 'num_epochs': 12, 'tree_depth': 7}\n",
            "Training model with learning rate: 9.999999999999999e-06, batch size: 16, epochs: 12, tree depth: 7\n",
            "Epoch [1/12], Training Loss: 0.7998 | Test Lost: 0.6923\n",
            "Epoch [2/12], Training Loss: 0.7854 | Test Lost: 0.6916\n",
            "Epoch [3/12], Training Loss: 0.7612 | Test Lost: 0.6924\n",
            "Epoch [4/12], Training Loss: 0.7618 | Test Lost: 0.6958\n",
            "Epoch [5/12], Training Loss: 0.7528 | Test Lost: 0.7042\n",
            "Epoch [6/12], Training Loss: 0.7490 | Test Lost: 0.7225\n",
            "Epoch [7/12], Training Loss: 0.7467 | Test Lost: 0.7650\n",
            "Epoch [8/12], Training Loss: 0.7270 | Test Lost: 0.8188\n",
            "Epoch [9/12], Training Loss: 0.7476 | Test Lost: 0.8603\n",
            "Epoch [10/12], Training Loss: 0.7406 | Test Lost: 0.8863\n",
            "Epoch [11/12], Training Loss: 0.7332 | Test Lost: 0.8855\n",
            "Epoch [12/12], Training Loss: 0.7215 | Test Lost: 0.8808\n",
            "Completed Trial 2/10.\n",
            "\n",
            "Trial 3/10: Hyperparameters - {'learning_rate': 9.999999999999999e-05, 'batch_size': 16, 'num_epochs': 14, 'tree_depth': 2}\n",
            "Training model with learning rate: 9.999999999999999e-05, batch size: 16, epochs: 14, tree depth: 2\n",
            "Epoch [1/14], Training Loss: 0.8273 | Test Lost: 0.6926\n",
            "Epoch [2/14], Training Loss: 0.8133 | Test Lost: 0.6959\n",
            "Epoch [3/14], Training Loss: 0.8005 | Test Lost: 0.7010\n",
            "Epoch [4/14], Training Loss: 0.7765 | Test Lost: 0.7102\n",
            "Epoch [5/14], Training Loss: 0.7744 | Test Lost: 0.7305\n",
            "Epoch [6/14], Training Loss: 0.7633 | Test Lost: 0.7629\n",
            "Epoch [7/14], Training Loss: 0.7709 | Test Lost: 0.7977\n",
            "Epoch [8/14], Training Loss: 0.7575 | Test Lost: 0.8221\n",
            "Epoch [9/14], Training Loss: 0.7383 | Test Lost: 0.8215\n",
            "Epoch [10/14], Training Loss: 0.7424 | Test Lost: 0.8019\n",
            "Epoch [11/14], Training Loss: 0.7324 | Test Lost: 0.7916\n",
            "Epoch [12/14], Training Loss: 0.7280 | Test Lost: 0.7757\n",
            "Epoch [13/14], Training Loss: 0.7246 | Test Lost: 0.7674\n",
            "Epoch [14/14], Training Loss: 0.7166 | Test Lost: 0.7626\n",
            "Completed Trial 3/10.\n",
            "\n",
            "Trial 4/10: Hyperparameters - {'learning_rate': 9.999999999999999e-06, 'batch_size': 32, 'num_epochs': 7, 'tree_depth': 8}\n",
            "Training model with learning rate: 9.999999999999999e-06, batch size: 32, epochs: 7, tree depth: 8\n",
            "Epoch [1/7], Training Loss: 0.9264 | Test Lost: 0.6897\n",
            "Epoch [2/7], Training Loss: 0.9052 | Test Lost: 0.6859\n",
            "Epoch [3/7], Training Loss: 0.8971 | Test Lost: 0.6834\n",
            "Epoch [4/7], Training Loss: 0.8997 | Test Lost: 0.6821\n",
            "Epoch [5/7], Training Loss: 0.8852 | Test Lost: 0.6857\n",
            "Epoch [6/7], Training Loss: 0.8928 | Test Lost: 0.6977\n",
            "Epoch [7/7], Training Loss: 0.8510 | Test Lost: 0.7319\n",
            "Completed Trial 4/10.\n",
            "\n",
            "Trial 5/10: Hyperparameters - {'learning_rate': 9.999999999999999e-06, 'batch_size': 16, 'num_epochs': 7, 'tree_depth': 6}\n",
            "Training model with learning rate: 9.999999999999999e-06, batch size: 16, epochs: 7, tree depth: 6\n",
            "Epoch [1/7], Training Loss: 0.7048 | Test Lost: 0.6882\n",
            "Epoch [2/7], Training Loss: 0.7052 | Test Lost: 0.6823\n",
            "Epoch [3/7], Training Loss: 0.7007 | Test Lost: 0.6729\n",
            "Epoch [4/7], Training Loss: 0.6989 | Test Lost: 0.6577\n",
            "Epoch [5/7], Training Loss: 0.6866 | Test Lost: 0.6369\n",
            "Epoch [6/7], Training Loss: 0.6822 | Test Lost: 0.6173\n",
            "Epoch [7/7], Training Loss: 0.6826 | Test Lost: 0.6112\n",
            "Completed Trial 5/10.\n",
            "\n",
            "Trial 6/10: Hyperparameters - {'learning_rate': 0.09999999999999999, 'batch_size': 32, 'num_epochs': 17, 'tree_depth': 2}\n",
            "Training model with learning rate: 0.09999999999999999, batch size: 32, epochs: 17, tree depth: 2\n",
            "Epoch [1/17], Training Loss: 0.6490 | Test Lost: 0.6842\n",
            "Epoch [2/17], Training Loss: 0.5530 | Test Lost: 0.6089\n",
            "Epoch [3/17], Training Loss: 0.5246 | Test Lost: 0.6052\n",
            "Epoch [4/17], Training Loss: 0.5610 | Test Lost: 0.6398\n",
            "Epoch [5/17], Training Loss: 0.5515 | Test Lost: 0.5416\n",
            "Epoch [6/17], Training Loss: 0.5380 | Test Lost: 0.5839\n",
            "Epoch [7/17], Training Loss: 0.5280 | Test Lost: 0.5892\n",
            "Epoch [8/17], Training Loss: 0.5315 | Test Lost: 0.5328\n",
            "Epoch [9/17], Training Loss: 0.5297 | Test Lost: 0.6198\n",
            "Epoch [10/17], Training Loss: 0.5312 | Test Lost: 0.5542\n",
            "Epoch [11/17], Training Loss: 0.5569 | Test Lost: 0.6239\n",
            "Epoch [12/17], Training Loss: 0.5462 | Test Lost: 0.6430\n",
            "Epoch [13/17], Training Loss: 0.5496 | Test Lost: 0.6061\n",
            "Epoch [14/17], Training Loss: 0.5245 | Test Lost: 0.5789\n",
            "Epoch [15/17], Training Loss: 0.5514 | Test Lost: 0.5909\n",
            "Epoch [16/17], Training Loss: 0.5191 | Test Lost: 0.5746\n",
            "Epoch [17/17], Training Loss: 0.5109 | Test Lost: 0.5592\n",
            "Completed Trial 6/10.\n",
            "\n",
            "Trial 7/10: Hyperparameters - {'learning_rate': 9.999999999999999e-05, 'batch_size': 16, 'num_epochs': 9, 'tree_depth': 8}\n",
            "Training model with learning rate: 9.999999999999999e-05, batch size: 16, epochs: 9, tree depth: 8\n",
            "Epoch [1/9], Training Loss: 0.6734 | Test Lost: 0.6602\n",
            "Epoch [2/9], Training Loss: 0.6159 | Test Lost: 0.6455\n",
            "Epoch [3/9], Training Loss: 0.5958 | Test Lost: 0.6420\n",
            "Epoch [4/9], Training Loss: 0.5660 | Test Lost: 0.6398\n",
            "Epoch [5/9], Training Loss: 0.5492 | Test Lost: 0.6358\n",
            "Epoch [6/9], Training Loss: 0.5475 | Test Lost: 0.6245\n",
            "Epoch [7/9], Training Loss: 0.5517 | Test Lost: 0.6124\n",
            "Epoch [8/9], Training Loss: 0.5411 | Test Lost: 0.5929\n",
            "Epoch [9/9], Training Loss: 0.5285 | Test Lost: 0.5876\n",
            "Completed Trial 7/10.\n",
            "\n",
            "Trial 8/10: Hyperparameters - {'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 5, 'tree_depth': 11}\n",
            "Training model with learning rate: 0.001, batch size: 32, epochs: 5, tree depth: 11\n",
            "Epoch [1/5], Training Loss: 0.6861 | Test Lost: 0.7614\n",
            "Epoch [2/5], Training Loss: 0.5739 | Test Lost: 0.7574\n",
            "Epoch [3/5], Training Loss: 0.5522 | Test Lost: 0.9037\n",
            "Epoch [4/5], Training Loss: 0.5505 | Test Lost: 0.6390\n",
            "Epoch [5/5], Training Loss: 0.5159 | Test Lost: 0.6823\n",
            "Completed Trial 8/10.\n",
            "\n",
            "Trial 9/10: Hyperparameters - {'learning_rate': 9.999999999999999e-06, 'batch_size': 16, 'num_epochs': 19, 'tree_depth': 11}\n",
            "Training model with learning rate: 9.999999999999999e-06, batch size: 16, epochs: 19, tree depth: 11\n",
            "Epoch [1/19], Training Loss: 0.7073 | Test Lost: 0.6655\n",
            "Epoch [2/19], Training Loss: 0.6773 | Test Lost: 0.6523\n",
            "Epoch [3/19], Training Loss: 0.6606 | Test Lost: 0.6488\n",
            "Epoch [4/19], Training Loss: 0.6397 | Test Lost: 0.6522\n",
            "Epoch [5/19], Training Loss: 0.6195 | Test Lost: 0.6592\n",
            "Epoch [6/19], Training Loss: 0.6120 | Test Lost: 0.6711\n",
            "Epoch [7/19], Training Loss: 0.6117 | Test Lost: 0.6870\n",
            "Epoch [8/19], Training Loss: 0.5976 | Test Lost: 0.6974\n",
            "Epoch [9/19], Training Loss: 0.6142 | Test Lost: 0.7011\n",
            "Epoch [10/19], Training Loss: 0.6044 | Test Lost: 0.6991\n",
            "Epoch [11/19], Training Loss: 0.5998 | Test Lost: 0.6962\n",
            "Epoch [12/19], Training Loss: 0.6031 | Test Lost: 0.6912\n",
            "Epoch [13/19], Training Loss: 0.6044 | Test Lost: 0.6885\n",
            "Epoch [14/19], Training Loss: 0.5923 | Test Lost: 0.6850\n",
            "Epoch [15/19], Training Loss: 0.5998 | Test Lost: 0.6849\n",
            "Epoch [16/19], Training Loss: 0.5824 | Test Lost: 0.6813\n",
            "Epoch [17/19], Training Loss: 0.5951 | Test Lost: 0.6768\n",
            "Epoch [18/19], Training Loss: 0.5940 | Test Lost: 0.6731\n",
            "Epoch [19/19], Training Loss: 0.5899 | Test Lost: 0.6719\n",
            "Completed Trial 9/10.\n",
            "\n",
            "Trial 10/10: Hyperparameters - {'learning_rate': 9.999999999999999e-05, 'batch_size': 16, 'num_epochs': 10, 'tree_depth': 11}\n",
            "Training model with learning rate: 9.999999999999999e-05, batch size: 16, epochs: 10, tree depth: 11\n",
            "Epoch [1/10], Training Loss: 0.6187 | Test Lost: 0.6940\n",
            "Epoch [2/10], Training Loss: 0.6204 | Test Lost: 0.6670\n",
            "Epoch [3/10], Training Loss: 0.5942 | Test Lost: 0.6512\n",
            "Epoch [4/10], Training Loss: 0.5714 | Test Lost: 0.6802\n",
            "Epoch [5/10], Training Loss: 0.5768 | Test Lost: 0.6548\n",
            "Epoch [6/10], Training Loss: 0.5556 | Test Lost: 0.6464\n",
            "Epoch [7/10], Training Loss: 0.5602 | Test Lost: 0.6424\n",
            "Epoch [8/10], Training Loss: 0.5365 | Test Lost: 0.6209\n",
            "Epoch [9/10], Training Loss: 0.5465 | Test Lost: 0.6009\n",
            "Epoch [10/10], Training Loss: 0.5394 | Test Lost: 0.6166\n",
            "Completed Trial 10/10.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "random_grid_search(model, hyperparameters, data[0], data[1], epochs=10, device='cpu', n_iter=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU29iDUHFVfV"
      },
      "outputs": [],
      "source": [
        "t = load_data('breast_cancer', framework='sklearn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78CEXuuCHc5R",
        "outputId": "adcb7576-0a8a-4353-c640-33e4a671b034"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
              "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0])"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t['train_y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOGcm_-DSek2"
      },
      "outputs": [],
      "source": [
        "{'learning_rate': 9.999999999999999e-05, 'batch_size': 16, 'num_epochs': 15, 'tree_depth': 7}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnAY4GkYHxA3"
      },
      "outputs": [],
      "source": [
        "model = SoftTreeEnsemble(num_trees=1,max_depth=7,\n",
        "                                 leaf_dims=2, input_dim=9, combine_output=True, subset_selection=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD5Sl6nbSpYn"
      },
      "outputs": [],
      "source": [
        "def testing(model, hyperparameters, train_loader, test_loader, device='cpu'):\n",
        "    \"\"\"\n",
        "    Perform random grid search on hyperparameters and train the model.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model.\n",
        "        hyperparameters: Dictionary of hyperparameters to search over.\n",
        "        train_loader: DataLoader for training.\n",
        "        test_loader: DataLoader for testing.\n",
        "        epochs: Number of epochs to train the model.\n",
        "        device: Device to use ('cpu' or 'cuda').\n",
        "        n_iter: Number of random hyperparameter combinations to try.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate all possible hyperparameter combinations (for reference)\n",
        "    hyperparameter_keys = list(hyperparameters.keys())\n",
        "\n",
        "\n",
        "    # Extract the sampled values\n",
        "\n",
        "    learning_rate = sample[\"learning_rate\"]\n",
        "    batch_size = sample[\"batch_size\"]\n",
        "    num_epochs = sample[\"num_epochs\"]\n",
        "    tree_depth = sample[\"tree_depth\"]\n",
        "    # num_trees = sample[\"num_trees\"]\n",
        "\n",
        "    # first thing is load the data\n",
        "    data = load_data('breast_cancer', framework='torch', batch_size=batch_size)\n",
        "\n",
        "    # create the instance of the model\n",
        "    model = SoftTreeEnsemble(num_trees=1,max_depth=tree_depth,\n",
        "                              leaf_dims=2, input_dim=9, combine_output=True, subset_selection=False)\n",
        "\n",
        "        # Train the model with the current hyperparameters\n",
        "        print(f\"Training model with learning rate: {learning_rate}, batch size: {batch_size}, \"\n",
        "              f\"epochs: {num_epochs}, tree depth: {tree_depth}\")\n",
        "\n",
        "        # Call the training function (your existing train_model function)\n",
        "        train_model(model, train_loader, test_loader, epochs=num_epochs, learning_rate=learning_rate, device=device)\n",
        "\n",
        "        # Add any additional evaluation or result saving here if needed\n",
        "        print(f\"Completed Trial {i+1}/{n_iter}.\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
